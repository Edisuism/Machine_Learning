{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Edisuism/Machine_Learning/blob/master/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y40Wnu5bCqy8",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwhHrbnmCys0",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1b1-Y0DDH6I",
        "colab_type": "text"
      },
      "source": [
        "The algorithm I have decided to build from scratch is an ID3 Decision tree. This algorithm takes in categorical variables such as a flower, it's colour and it's size. It only functions with categorical variables as each unique value of the attributes become a target in which to create a subtable from which can cause significant overfitting and an unmeaningful and unflexible decision tree when dealing with attributes with many uniqeu values such as age or income (if they are not preprocessed such as by binning). As output, the algorithm will determine based on the attributes given and the target variable it has been tasked with estimating, what the target variable is most likely to be and returning a unique value found underneath the target variable column. In the case of the flower it could determine what type of flower it is based on it's attributes. This works by first determining the most impactful attribute in deciding the variable type as the root node and then repeating the process for all the other attributes, identifying the effects of each attribute on the likelihood of it resulting in being any given flower.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjOWe-mpC56K",
        "colab_type": "text"
      },
      "source": [
        "# Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK35b55XLTLw",
        "colab_type": "text"
      },
      "source": [
        "Some Challenges I find in creating this algorithm is figuring out how to turn the theory into practical, functional code such as calculating entropy with respects to each attribute and it's result on the target variable while ensuring it repeats the process for each layer of the decision tree.\n",
        "\n",
        "\n",
        "My current plan for the project is to split the dataframe into a training and test set to ensure I will be able to test the algorithm without any additional data. \n",
        "\n",
        "\n",
        "As for the algorithm itself, I will calculate the entropy of the target variable of the data set and then for every attribute with the addition of also calculating the information gain of the attributes by subtracting the entropy from the entropy of the target variable. After calculating the information gain for each attribute, the algorithm will choose the highest value as the root node and then cull all pure results from the training dataframe, that is, if all the results are to play tennis if the outlook is overcast, all those rows will be removed from the table. The algorithm will then create a subtable underneath unpure data and recalculate the new entropy of the target variables and each attribute as it did to find the root node. This process of calculating entropy of the target variable and the information gain of attributes will repeat until the data is pure.\n",
        "\n",
        "\n",
        "However, what happens in the case that the tree has already reached the end of it's possibilities (taking note that the tree will not go up a level again in order to test out new possibilities)? I am currently thinking of simply semi-randomising it's estimation of the target variable based on the quantity of the results e.g. if at the end of the tree, there is still 1 result saying to play tennis and 1 result saying to not play tennis, then the answer will have a 50% chance to be either play or not play.\n",
        "\n",
        "\n",
        "I will test the accuracy of the algorithm by comparing the predictions made by the decision tree on the test set with the actual values of the test set as a percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgoAZ--C9lR",
        "colab_type": "text"
      },
      "source": [
        "# Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByoxS30g0QrK",
        "colab_type": "text"
      },
      "source": [
        "Import data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCqWnuyEYZs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import log2 as log\n",
        "import random\n",
        "import pprint\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/Edisuism/Machine_Learning/master/play_tennis.csv'\n",
        "df = pd.read_csv(url)\n",
        "print (len(df))\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVHi95HI0paY",
        "colab_type": "text"
      },
      "source": [
        "Split into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCOfjN-M0omX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(df, test_size):\n",
        "  test_size = round(test_size * len(df)) \n",
        "  indices = df.index.tolist()\n",
        "  \n",
        "  #randomly select samples based on test_size\n",
        "  test_indices = random.sample(population = indices, k = test_size)\n",
        "  \n",
        "  #create test and training set\n",
        "  test_df = df.loc[test_indices]\n",
        "  train_df = df.drop(test_indices)\n",
        "  \n",
        "  return train_df, test_df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D78itKcu29oI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df, test_df = train_test_split(df, test_size = 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxMOPEAx4Tcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (len(train_df))\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta_q1Gyz4KJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (len(test_df))\n",
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLONyUDzLb-J",
        "colab_type": "text"
      },
      "source": [
        "Find Entropy of Class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTfYWwM5UkQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate the entropy of the class\n",
        "def class_entropy(df):\n",
        "  \n",
        "#target is the column we are interested in  \n",
        "  target = df.keys()[-1]\n",
        "  entropy_class = 0  \n",
        "  \n",
        "#get possible values for the target, yes and no\n",
        "  values = df[target].unique()  \n",
        "  for value in values:\n",
        "      fraction = df[target].value_counts()[value] / len(df[target])  \n",
        "      entropy_class += -fraction * np.log2(fraction)\n",
        "  return entropy_class\n",
        "\n",
        "class_entropy(train_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHVg-wXTM54z",
        "colab_type": "text"
      },
      "source": [
        "Find Entropy and Information Gain of Attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY6_xT_oM-Y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate entropy of each attribute\n",
        "def attribute_entropy(df,target_attribute):\n",
        "  target = df.keys()[-1]\n",
        "  attribute = target_attribute\n",
        "#in case of a 0 denominator to prevent error\n",
        "  eps = np.finfo(float).eps \n",
        "  \n",
        "#get unique values of the chosen attribute and calculate entropy based on the variance in target\n",
        "  target_variables = df[target].unique() \n",
        "  variables = df[attribute].unique()    \n",
        "  entropy_attribute = 0\n",
        "  for variable in variables:\n",
        "      entropy_in_each = 0\n",
        "      for target_variable in target_variables:\n",
        "          num = len(df[attribute][df[attribute] == variable][df[target] == target_variable]) \n",
        "          den = len(df[attribute][df[attribute] == variable])  \n",
        "          fraction = num/(den+eps)  \n",
        "          \n",
        "#entropy for one feature \n",
        "          entropy_in_each += -fraction * log(fraction + eps) \n",
        "      fraction2 = den/len(df)\n",
        "      \n",
        "#all the entropy for attribute\n",
        "      entropy_attribute += -fraction2 * entropy_in_each  \n",
        "  E_final = abs(entropy_attribute)\n",
        "  return E_final\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU3aG8rahrtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (attribute_entropy(train_df, 'outlook'))\n",
        "print (attribute_entropy(train_df, 'temp'))\n",
        "print (attribute_entropy(train_df, 'humidity'))\n",
        "print (attribute_entropy(train_df, 'wind'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfcj0LVgRsvM",
        "colab_type": "text"
      },
      "source": [
        "Find Highest Info Gain Attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk0TK8lJRxLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate and compare information gain of each attribute and returns the one with the highest value\n",
        "def calculate_winner(df):\n",
        "  IG = []\n",
        "  \n",
        "#1 to class because [0] is just an index\n",
        "  for key in df.keys()[1:-1]: \n",
        "    IG.append(class_entropy(df) - attribute_entropy(df,key))\n",
        "  \n",
        "  return df.keys()[1:-1][np.argmax(IG)]\n",
        "\n",
        "calculate_winner(train_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7Y3XXzsY9U-",
        "colab_type": "text"
      },
      "source": [
        "Splitting function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI7WrJXlY_jL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function that can be repeatedly called to split the table\n",
        "def split_table(df, node, value):\n",
        "  return df[df[node] == value].reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RY-slltWrAY",
        "colab_type": "text"
      },
      "source": [
        "Build Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka0M3O-0WsMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buildTree(df,tree=None): \n",
        "    target = df.keys()[-1] \n",
        "    \n",
        "#calculate root node based on current table\n",
        "    node = calculate_winner(df)\n",
        "    attValue = np.unique(df[node])\n",
        "    \n",
        "#create tree\n",
        "    if tree is None:                    \n",
        "        tree={}\n",
        "        tree[node] = {}\n",
        "        \n",
        "#create a new subtable for each attribute value\n",
        "    for value in attValue:\n",
        "        subtable = split_table(df, node, value)\n",
        "        clValue, counts = np.unique(subtable[target], return_counts=True)                        \n",
        "        \n",
        "#when data is pure then stop otherwise continue to build the tree    \n",
        "        if len(counts)==1:\n",
        "            tree[node][value] = clValue[0]                                                    \n",
        "        else:        \n",
        "            tree[node][value] = buildTree(subtable)\n",
        "                   \n",
        "    return tree\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9If0W6cYZEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decision_tree = buildTree(train_df)\n",
        "pprint.pprint(decision_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXt-ZsIagJvh",
        "colab_type": "text"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyiDjAyMgLDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(inst,tree):\n",
        "    for nodes in tree.keys():        \n",
        "        value = inst[nodes]\n",
        "        tree = tree[nodes][value]\n",
        "        \n",
        "#default prediction\n",
        "        prediction = 0\n",
        "        \n",
        "#find corresponding values in decision tree\n",
        "        if type(tree) is dict:\n",
        "            prediction = predict(inst, tree)\n",
        "        else:\n",
        "            prediction = tree\n",
        "            break;                            \n",
        "        \n",
        "    return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycSAPAs43gVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_test(df, tree):\n",
        "#create new list to store predictions\n",
        "  prediction_list = []\n",
        "  i=0\n",
        "  \n",
        "#for each row, get the prediction from the tree based on attributes and add to list\n",
        "  for i in range (len(df.index)):\n",
        "    row = df.iloc[i]\n",
        "    prediction = predict(row,tree)\n",
        "    prediction_list.append(prediction)\n",
        "  \n",
        "  return prediction_list\n",
        "\n",
        "predict_test(test_df, decision_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MTh6KzvpHyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(df, predicted):\n",
        "  target = df.keys()[-1]\n",
        "  \n",
        "#get the values of the target variable in list form\n",
        "  actual = test_df[target].tolist()\n",
        "  correct = 0\n",
        "  \n",
        "#compare actual values of dataframe to predicted values of dataframe\n",
        "  for i in range (len(df.index)):\n",
        "    if actual[i] == predicted[i]:\n",
        "      correct += 1      \n",
        "      \n",
        "#calculate the percentage of predictions that were correct \n",
        "  return correct / float(len(actual)) * 100.0\n",
        "\n",
        "accuracy(test_df, predict_test(test_df, decision_tree))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi__evFcC_yR",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnr97QXzGDg8",
        "colab_type": "text"
      },
      "source": [
        "In order to test the capabilities and reliabilty of my algorithm I designed a sequence of functions that could be called in order to determine the percentage of predictions made by the decision tree algorithm that were correct by comparing them to the actual result. This was done by initally splitting the data into training and testing sets, training the algorithm using the training set and then attempting to predict the target variable of the testing set. By turning the target variables in the testing set as well as the predictions of the decision tree, they could be compared with one another to form an accuracy percentage.\n",
        "\n",
        "Initial testing demonstrated the algorithm to be quite inconsistant with the play tennis achieving the following results [100, 50, 50, 100, 100]% during testing. This significant variance in results was likely to a small dataset however. Moving to a larger dataset 'titanic' should resolve this problem however crashes would occur as it became obvious that my algorithm was unsuited to datasets that are not categorical. It is possible to still apply this algorithm to non-categorical data but preprocessing will be required"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8y0PAD1DCLp",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2qkxAnHflJD",
        "colab_type": "text"
      },
      "source": [
        "This was an initial attempt at creating a machine learning algorithm to the same complexity as an ID3 decision tree learner and it was very challenging and the end product was far from perfect. It fails to combat situations where results in the end are not pure for example if a... sunny, windy, dry day happened and there were results saying that you could both play and not play, the algorithm will loop on ad infinitum. The paramaters of the algorithm as of now is essentially unadjustable and there are no advanced features to optimise speed and performance and as such future improvements could involve adjusting code to allow for parameters to be changed in order to give more control to users. In order to optimise the algorithm, I also feel that running multiple decision trees would help achieve a higher level of accuracy though this will require more study in order to accomplish."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJzMM0vDE0X",
        "colab_type": "text"
      },
      "source": [
        "# Ethical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCqIPuXJB0z8",
        "colab_type": "text"
      },
      "source": [
        "My algorithm in it's simplest form is taking in statistical information as input and then creating a prediction based on them as output. This can have many applications such as in determining a person's likelihood to commit crime and also determine the driving patterns of self-driving vehicles. In these scenarios, many ethical misuses of the algorithm could be made such as giving greater weight to certain attributes as well as the manipulation of input data to create biased decision trees in order to support a certain purpose. As an example, a person with brown hair who plans to steal may reduce the weighting of brown hair in determining one's propensity to commit crime or sample from biased data which results in brown hair having no or less impact on one's propensity to commit crime. By following the ethical framework of Utilitarianism where communal happiness forms moral decisions, we are able to deem this misuse unethical. This is because by altering the decision making process of the machine to favor brown haired people, more statistically likely people to commit crime are getting away with criminal activity resulting in a less stable and ultimately less happy society. This method of thinking however is not perfect as 'level of happiness' a hard thing to measure especially with the action of tampering with a machine learning algorithm as the results of doing so are not obviously clear. It is then important to patch up such grey areas with the Contractarianism framework which declares morality as that which a group determines to be so. By first valueing universal net happiness first and then falling back onto hard contracts next, morals become very clear. Applying this to aforementioned 'brown hair' example... by changing the algorithm in favor of brown haired people, though it is hard to determine whether it creates the maximum net happiness, the person tampering with the algorithm is likely in breach of company contracts that prevent personal intervention and thus his action is deemed unethical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfSDW6TY3tbS",
        "colab_type": "text"
      },
      "source": [
        "# Video Link\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amkFsLEB346b",
        "colab_type": "text"
      },
      "source": [
        "https://youtu.be/V3BDfyjwS38"
      ]
    }
  ]
}