{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Edisuism/Machine_Learning/blob/master/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y40Wnu5bCqy8",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0nu7t0NDOs7",
        "colab_type": "text"
      },
      "source": [
        "In this page, I will develop a machine learning algorithm from scratch and then test it on a sample dataset\n",
        "\n",
        "Steps involved:\n",
        "- Find sample dataset\n",
        "- Choose learner algorithm and note down input/output\n",
        "- Preprocess sample dataset to input requirements and split into test/training sets \n",
        "- code learner algorithm **(only this needs to be coded, preprocessing can use from libaries like SKlearn)**\n",
        "- train algorithm and then apply to test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwhHrbnmCys0",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1b1-Y0DDH6I",
        "colab_type": "text"
      },
      "source": [
        "The algorithm I have decided to build from scratch is an ID3 Decision tree. This algorithm takes in variables as well in addition to their attributes such as a flower and it's colour and size. As output, the algorithm will determine based on the attributes given and the target variable it has been tasked with estimating, what the target variable is. In the case of the flower it could determine what type of flower it is based on it's attributes. This works by first determining the most impactful attribute in deciding the variable type as the root node and then repeating the process for all the other attributes, achieving a greater level of classification each time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjOWe-mpC56K",
        "colab_type": "text"
      },
      "source": [
        "# Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK35b55XLTLw",
        "colab_type": "text"
      },
      "source": [
        "Some Challenges I find in creating this algorithm is figuring out how to turn the theory into practical, functional code such as calculating entropy with respects to each attribute and it's result on the target variable while ensuring it repeats the process for each layer of the decision tree.\n",
        "\n",
        "\n",
        "My current plan for the project is to split the dataframe into a training and test set to ensure I will be able to test the algorithm without any additional data. \n",
        "\n",
        "\n",
        "As for the algorithm itself, I will calculate the entropy of the target variable of the data set and then for every attribute with the addition of also calculating the information gain of the attributes by subtracting the entropy from the entropy of the target variable. After calculating the information gain for each attribute, the algorithm will choose the highest value as the root node and then cull all pure results from the training dataframe, that is, if all the results are to play tennis if the outlook is overcast, all those rows will be removed from the table. The algorithm will then recalculate the new entropy of the target variables and each attribute as it did to find the root node. This process of calculating entropy of the target variable and the information gain of attributes will repeat until the data is pure.\n",
        "\n",
        "\n",
        "However, what happens in the case that the tree has already reached the end of it's possibilities (taking note that the tree will not go up a level again in order to test out new possibilities)? I am currently thinking of simply semi-randomising it's estimation of the target variable based on the quantity of the results e.g. if at the end of the tree, there is still 1 result saying to play tennis and 1 result saying to not play tennis, then the answer will have a 50% chance to be either play or not play."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgoAZ--C9lR",
        "colab_type": "text"
      },
      "source": [
        "# Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByoxS30g0QrK",
        "colab_type": "text"
      },
      "source": [
        "Import data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCqWnuyEYZs1",
        "colab_type": "code",
        "outputId": "42ee5b94-169d-4d28-9759-3eb36a9b8b35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import log2 as log\n",
        "import random\n",
        "import pprint\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/Edisuism/Machine_Learning/master/play_tennis.csv'\n",
        "df = pd.read_csv(url)\n",
        "print (len(df))\n",
        "df.head()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>outlook</th>\n",
              "      <th>temp</th>\n",
              "      <th>humidity</th>\n",
              "      <th>wind</th>\n",
              "      <th>play</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>D1</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>Hot</td>\n",
              "      <td>High</td>\n",
              "      <td>Weak</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>D2</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>Hot</td>\n",
              "      <td>High</td>\n",
              "      <td>Strong</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>D3</td>\n",
              "      <td>Overcast</td>\n",
              "      <td>Hot</td>\n",
              "      <td>High</td>\n",
              "      <td>Weak</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>D4</td>\n",
              "      <td>Rain</td>\n",
              "      <td>Mild</td>\n",
              "      <td>High</td>\n",
              "      <td>Weak</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>D5</td>\n",
              "      <td>Rain</td>\n",
              "      <td>Cool</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Weak</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  day   outlook  temp humidity    wind play\n",
              "0  D1     Sunny   Hot     High    Weak   No\n",
              "1  D2     Sunny   Hot     High  Strong   No\n",
              "2  D3  Overcast   Hot     High    Weak  Yes\n",
              "3  D4      Rain  Mild     High    Weak  Yes\n",
              "4  D5      Rain  Cool   Normal    Weak  Yes"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVHi95HI0paY",
        "colab_type": "text"
      },
      "source": [
        "Split into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCOfjN-M0omX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(df, test_size):\n",
        "  test_size = round(test_size * len(df)) \n",
        "  indices = df.index.tolist()\n",
        "  \n",
        "  #randomly select samples based on test_size\n",
        "  test_indices = random.sample(population = indices, k = test_size)\n",
        "  \n",
        "  #create test and training set\n",
        "  test_df = df.loc[test_indices]\n",
        "  train_df = df.drop(test_indices)\n",
        "  \n",
        "  return train_df, test_df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D78itKcu29oI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df, test_df = train_test_split(df, test_size = 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxMOPEAx4Tcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (len(train_df))\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta_q1Gyz4KJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (len(test_df))\n",
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLONyUDzLb-J",
        "colab_type": "text"
      },
      "source": [
        "Find Entropy of Class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTfYWwM5UkQH",
        "colab_type": "code",
        "outputId": "f253793a-0ec1-4ac2-de5d-9017995436e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#calculate the entropy of the class\n",
        "def class_entropy(df):\n",
        "  \n",
        "#target is the column we are interested in  \n",
        "  target = df.keys()[-1]\n",
        "  entropy_class = 0  \n",
        "  \n",
        "#get possible values for the target, yes and no\n",
        "  values = df[target].unique()  \n",
        "  for value in values:\n",
        "      fraction = df[target].value_counts()[value]/len(df[target])  \n",
        "      entropy_class += -fraction*np.log2(fraction)\n",
        "  return entropy_class\n",
        "\n",
        "class_entropy(train_df)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHVg-wXTM54z",
        "colab_type": "text"
      },
      "source": [
        "Find Entropy and Information Gain of Attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY6_xT_oM-Y_",
        "colab_type": "code",
        "outputId": "dee2e837-6f6b-4bab-d575-bf4e7efbde53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#calculate entropy of each attribute\n",
        "def attribute_entropy(df,target_attribute):\n",
        "  target = df.keys()[-1]\n",
        "  attribute = target_attribute\n",
        "#in case of a 0 denominator to prevent error\n",
        "  eps = np.finfo(float).eps \n",
        "  \n",
        "#get unique values of the chosen attribute and calculate entropy based on the variance in target\n",
        "  target_variables = df[target].unique() \n",
        "  variables = df[attribute].unique()    \n",
        "  entropy_attribute = 0\n",
        "  for variable in variables:\n",
        "      entropy_each_feature = 0\n",
        "      for target_variable in target_variables:\n",
        "          num = len(df[attribute][df[attribute]==variable][df[target] ==target_variable]) \n",
        "          den = len(df[attribute][df[attribute]==variable])  \n",
        "          fraction = num/(den+eps)  \n",
        "          \n",
        "#entropy for one feature \n",
        "          entropy_each_feature += -fraction*log(fraction+eps) \n",
        "      fraction2 = den/len(df)\n",
        "      \n",
        "#all the entropy for attribute\n",
        "      entropy_attribute += -fraction2*entropy_each_feature  \n",
        "  E_final = abs(entropy_attribute)\n",
        "  return E_final\n",
        "\n",
        "print (attribute_entropy(train_df, 'outlook'))\n",
        "print (attribute_entropy(train_df, 'temp'))\n",
        "print (attribute_entropy(train_df, 'humidity'))\n",
        "print (attribute_entropy(train_df, 'wind'))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3245112497836527\n",
            "0.9999999999999993\n",
            "0.8754887502163462\n",
            "0.9999999999999993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfcj0LVgRsvM",
        "colab_type": "text"
      },
      "source": [
        "Find Highest Info Gain Attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk0TK8lJRxLu",
        "colab_type": "code",
        "outputId": "38d74cdc-634a-4d04-9d39-2a2826832c6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#calculate and compare information gain of each attribute and returns the one with the highest value\n",
        "def calculate_winner(df):\n",
        "  IG = []\n",
        "  \n",
        "#1 to class because [0] is just an index\n",
        "  for key in df.keys()[1:-1]: \n",
        "    IG.append(class_entropy(df)-attribute_entropy(df,key))\n",
        "  \n",
        "  return df.keys()[1:-1][np.argmax(IG)]\n",
        "\n",
        "calculate_winner(train_df)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'outlook'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7Y3XXzsY9U-",
        "colab_type": "text"
      },
      "source": [
        "Splitting function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI7WrJXlY_jL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function that can be repeatedly called to split the table\n",
        "def split_table(df, node, value):\n",
        "  return df[df[node] == value].reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RY-slltWrAY",
        "colab_type": "text"
      },
      "source": [
        "Build Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka0M3O-0WsMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buildTree(df,tree=None): \n",
        "    target = df.keys()[-1] \n",
        "    \n",
        "#calculate root node based on current table\n",
        "    node = calculate_winner(df)\n",
        "    attValue = np.unique(df[node])\n",
        "    \n",
        "#create tree\n",
        "    if tree is None:                    \n",
        "        tree={}\n",
        "        tree[node] = {}\n",
        "        \n",
        "#create a new subtable for each attribute value\n",
        "    for value in attValue:\n",
        "        subtable = split_table(df, node, value)\n",
        "        clValue, counts = np.unique(subtable[target],return_counts=True)                        \n",
        "        \n",
        "#when data is pure then stop otherwise continue to build the tree    \n",
        "        if len(counts)==1:\n",
        "            tree[node][value] = clValue[0]                                                    \n",
        "        else:        \n",
        "            tree[node][value] = buildTree(subtable)\n",
        "                   \n",
        "    return tree\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9If0W6cYZEt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "1f8040b9-ae5b-4282-d8fe-44ffcc835d5b"
      },
      "source": [
        "decision_tree = buildTree(train_df)\n",
        "pprint.pprint(decision_tree)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'humidity': {'High': {'outlook': {'Overcast': 'Yes',\n",
            "                                   'Rain': {'wind': {'Strong': 'No',\n",
            "                                                     'Weak': 'Yes'}},\n",
            "                                   'Sunny': 'No'}},\n",
            "              'Normal': 'Yes'}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXt-ZsIagJvh",
        "colab_type": "text"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyiDjAyMgLDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(inst,tree):\n",
        "    for nodes in tree.keys():        \n",
        "        value = inst[nodes]\n",
        "        tree = tree[nodes][value]\n",
        "        \n",
        "#default prediction\n",
        "        prediction = 0\n",
        "        \n",
        "#find corresponding values in decision tree\n",
        "        if type(tree) is dict:\n",
        "            prediction = predict(inst, tree)\n",
        "        else:\n",
        "            prediction = tree\n",
        "            break;                            \n",
        "        \n",
        "    return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycSAPAs43gVS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81d52363-63b8-4bc9-c2bd-089251d665a4"
      },
      "source": [
        "def predict_test(df, tree):\n",
        "#create new list to store predictions\n",
        "  prediction_list = []\n",
        "  i=0\n",
        "  \n",
        "#for each row, get the prediction from the tree based on attributes and add to list\n",
        "  for i in range (len(df.index)):\n",
        "    row = df.iloc[i]\n",
        "    prediction = predict(row,tree)\n",
        "    prediction_list.append(prediction)\n",
        "  \n",
        "  return prediction_list\n",
        "\n",
        "predict_test(test_df, decision_tree)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Yes', 'Yes', 'Yes', 'Yes']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MTh6KzvpHyv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa9d4894-c36d-42b5-ba1d-9fe048d2de71"
      },
      "source": [
        "def accuracy(df, predicted):\n",
        "  target = df.keys()[-1]\n",
        "  \n",
        "#get the values of the target variable in list form\n",
        "  actual = test_df[target].tolist()\n",
        "  correct = 0\n",
        "  \n",
        "#compare actual values of dataframe to predicted values of dataframe\n",
        "  for i in range (len(df.index)):\n",
        "    if actual[i] == predicted[i]:\n",
        "      correct += 1      \n",
        "      \n",
        "#calculate the percentage of predictions that were correct \n",
        "  return correct / float(len(actual)) * 100.0\n",
        "\n",
        "accuracy(test_df, predict_test(test_df, decision_tree))"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pc5XGKaYX6_",
        "colab_type": "text"
      },
      "source": [
        "Notes:\n",
        "Gini index = how much uncertainty there is in a node\n",
        ",Information gain = how much uncertainty is removed in a node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi__evFcC_yR",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnr97QXzGDg8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8y0PAD1DCLp",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2qkxAnHflJD",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJzMM0vDE0X",
        "colab_type": "text"
      },
      "source": [
        "# Ethical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCqIPuXJB0z8",
        "colab_type": "text"
      },
      "source": [
        "My algorithm in it's simplest form is taking in statistical information as input and then creating a prediction based on them as output. This can have many applications such as in determining a person's likelihood to commit crime and also determine the driving patterns of self-driving vehicles. In these scenarios, many ethical misuses of the algorithm could be made such as giving greater weight to certain attributes as well as the manipulation of input data to create biased decision trees in order to support a certain purpose. As an example, a person with brown hair who plans to steal may reduce the weighting of brown hair in determining one's propensity to commit crime or sample from biased data which results in brown hair having no or less impact on one's propensity to commit crime. By following the ethical framework of Utilitarianism where communal happiness forms moral decisions, we are able to deem this misuse unethical. This is because by altering the decision making process of the machine to favor brown haired people, more statistically likely people to commit crime are getting away with criminal activity resulting in a less stable and ultimately less happy society. This method of thinking however is not perfect as 'level of happiness' a hard thing to measure especially with the action of tampering with a machine learning algorithm as the results of doing so are not obviously clear. It is then important to patch up such grey areas with the Contractarianism framework which declares morality as that which a group determines to be so. By first valueing universal net happiness first and then falling back onto hard contracts next, morals become very clear. Applying this to aforementioned 'brown hair' example... by changing the algorithm in favor of brown haired people, though it is hard to determine whether it creates the maximum net happiness, the person tampering with the algorithm is likely in breach of company contracts that prevent personal intervention and thus his action is deemed unethical."
      ]
    }
  ]
}