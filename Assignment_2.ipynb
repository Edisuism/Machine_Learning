{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Edisuism/Machine_Learning/blob/master/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y40Wnu5bCqy8",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0nu7t0NDOs7",
        "colab_type": "text"
      },
      "source": [
        "In this page, I will develop a machine learning algorithm from scratch and then test it on a sample dataset\n",
        "\n",
        "Steps involved:\n",
        "- Find sample dataset\n",
        "- Choose learner algorithm and note down input/output\n",
        "- Preprocess sample dataset to input requirements and split into test/training sets \n",
        "- code learner algorithm **(only this needs to be coded, preprocessing can use from libaries like SKlearn)**\n",
        "- train algorithm and then apply to test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwhHrbnmCys0",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1b1-Y0DDH6I",
        "colab_type": "text"
      },
      "source": [
        "The algorithm I have decided to build from scratch is an ID3 Decision tree. This algorithm takes in variables as well in addition to their attributes such as a flower and it's colour and size. As output, the algorithm will determine based on the attributes given and the target variable it has been tasked with estimating, what the target variable is. In the case of the flower it could determine what type of flower it is based on it's attributes. This works by first determining the most impactful attribute in deciding the variable type as the root node and then repeating the process for all the other attributes, achieving a greater level of classification each time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjOWe-mpC56K",
        "colab_type": "text"
      },
      "source": [
        "# Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK35b55XLTLw",
        "colab_type": "text"
      },
      "source": [
        "Some Challenges I find in creating this algorithm is figuring out how to turn the theory into practical, functional code such as calculating entropy with respects to each attribute and it's result on the target variable while ensuring it repeats the process for each layer of the decision tree.\n",
        "\n",
        "\n",
        "My current plan for the project is to split the dataframe into a training and test set to ensure I will be able to test the algorithm without any additional data. \n",
        "\n",
        "\n",
        "As for the algorithm itself, I will calculate the entropy of the target variable of the data set and then for every attribute with the addition of also calculating the information gain of the attributes by subtracting the entropy from the entropy of the target variable. After calculating the information gain for each attribute, the algorithm will choose the highest value as the root node and then cull all pure results from the training dataframe, that is, if all the results are to play tennis if the outlook is overcast, all those rows will be removed from the table. The algorithm will then recalculate the new entropy of the target variables and each attribute as it did to find the root node. This process of calculating entropy of the target variable and the information gain of attributes will repeat until the data is pure.\n",
        "\n",
        "\n",
        "However, what happens in the case that the tree has already reached the end of it's possibilities (taking note that the tree will not go up a level again in order to test out new possibilities)? I am currently thinking of simply semi-randomising it's estimation of the target variable based on the quantity of the results e.g. if at the end of the tree, there is still 1 result saying to play tennis and 1 result saying to not play tennis, then the answer will have a 50% chance to be either play or not play."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgoAZ--C9lR",
        "colab_type": "text"
      },
      "source": [
        "# Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByoxS30g0QrK",
        "colab_type": "text"
      },
      "source": [
        "Import data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCqWnuyEYZs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import log2 as log\n",
        "import random\n",
        "import pprint\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/Edisuism/Machine_Learning/master/play_tennis.csv'\n",
        "df = pd.read_csv(url)\n",
        "print (len(df))\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVHi95HI0paY",
        "colab_type": "text"
      },
      "source": [
        "Split into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCOfjN-M0omX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(df, test_size):\n",
        "  test_size = round(test_size * len(df))\n",
        "  indices = df.index.tolist()\n",
        "  test_indices = random.sample(population = indices, k = test_size)\n",
        "  test_df = df.loc[test_indices]\n",
        "  train_df = df.drop(test_indices)\n",
        "  \n",
        "  return train_df, test_df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D78itKcu29oI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df, test_df = train_test_split(df, test_size = 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxMOPEAx4Tcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (len(train_df))\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta_q1Gyz4KJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (len(test_df))\n",
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLONyUDzLb-J",
        "colab_type": "text"
      },
      "source": [
        "Find Entropy of Class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTfYWwM5UkQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def class_entropy(df):\n",
        "  target = df.keys()[-1]\n",
        "  entropy_class = 0  \n",
        "  values = df[target].unique()  \n",
        "  for value in values:\n",
        "      fraction = df[target].value_counts()[value]/len(df[target])  \n",
        "      entropy_class += -fraction*np.log2(fraction)\n",
        "  return entropy_class\n",
        "\n",
        "class_entropy(train_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHVg-wXTM54z",
        "colab_type": "text"
      },
      "source": [
        "Find Entropy and Information Gain of Attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY6_xT_oM-Y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attribute_entropy(df,target_attribute):\n",
        "  target = df.keys()[-1]\n",
        "  attribute = target_attribute\n",
        "  eps = np.finfo(float).eps #In case of 0 denominator\n",
        "  target_variables = df[target].unique() \n",
        "  variables = df[attribute].unique()    \n",
        "  entropy_attribute = 0\n",
        "  for variable in variables:\n",
        "      entropy_each_feature = 0\n",
        "      for target_variable in target_variables:\n",
        "          num = len(df[attribute][df[attribute]==variable][df[target] ==target_variable]) \n",
        "          den = len(df[attribute][df[attribute]==variable])  \n",
        "          fraction = num/(den+eps)  \n",
        "          entropy_each_feature += -fraction*log(fraction+eps) #entropy for one feature \n",
        "      fraction2 = den/len(df)\n",
        "      entropy_attribute += -fraction2*entropy_each_feature   #all the entropy for attribute\n",
        "  E_final = abs(entropy_attribute)\n",
        "  return E_final\n",
        "\n",
        "print (attribute_entropy(train_df, 'outlook'))\n",
        "print (attribute_entropy(train_df, 'temp'))\n",
        "print (attribute_entropy(train_df, 'humidity'))\n",
        "print (attribute_entropy(train_df, 'wind'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfcj0LVgRsvM",
        "colab_type": "text"
      },
      "source": [
        "Find Highest Info Gain Attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk0TK8lJRxLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_winner(df):\n",
        "  IG = []\n",
        "  for key in df.keys()[1:-1]: #1 to class because [0] is just an index\n",
        "    IG.append(class_entropy(df)-attribute_entropy(df,key))\n",
        "  \n",
        "  return df.keys()[1:-1][np.argmax(IG)]\n",
        "\n",
        "calculate_winner(train_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7Y3XXzsY9U-",
        "colab_type": "text"
      },
      "source": [
        "Splitting function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI7WrJXlY_jL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_table(df, node, value):\n",
        "  return df[df[node] == value].reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RY-slltWrAY",
        "colab_type": "text"
      },
      "source": [
        "Build Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka0M3O-0WsMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buildTree(df,tree=None): \n",
        "    target = df.keys()[-1] \n",
        "    node = calculate_winner(df)\n",
        "    attValue = np.unique(df[node])\n",
        "    if tree is None:                    \n",
        "        tree={}\n",
        "        tree[node] = {}\n",
        "\n",
        "    for value in attValue:\n",
        "        \n",
        "        subtable = split_table(df, node, value)\n",
        "        clValue, counts = np.unique(subtable[target],return_counts=True)                        \n",
        "        \n",
        "        if len(counts)==1:\n",
        "            tree[node][value] = clValue[0]                                                    \n",
        "        else:        \n",
        "            tree[node][value] = buildTree(subtable)\n",
        "                   \n",
        "    return tree\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9If0W6cYZEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decision_tree = buildTree(train_df)\n",
        "pprint.pprint(decision_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXt-ZsIagJvh",
        "colab_type": "text"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyiDjAyMgLDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(inst,tree):\n",
        "    for nodes in tree.keys():        \n",
        "        value = inst[nodes]\n",
        "        tree = tree[nodes][value]\n",
        "        prediction = 0\n",
        "            \n",
        "        if type(tree) is dict:\n",
        "            prediction = predict(inst, tree)\n",
        "        else:\n",
        "            prediction = tree\n",
        "            break;                            \n",
        "        \n",
        "    return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycSAPAs43gVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decision_tree.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPq5ZmLBgtBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inst = test_df.iloc[1]\n",
        "inst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE0bJ15mhEr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = predict(inst,decision_tree)\n",
        "prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcbxjRoew-d8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(df, actual, predicted):\n",
        "\tcorrect = 0\n",
        "  for index, row in df.iterrows(): \n",
        "    for i in range(len(actual)):\n",
        "      if actual[i] == predicted[i]:\n",
        "        correct += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pc5XGKaYX6_",
        "colab_type": "text"
      },
      "source": [
        "Notes:\n",
        "Gini index = how much uncertainty there is in a node\n",
        ",Information gain = how much uncertainty is removed in a node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi__evFcC_yR",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8y0PAD1DCLp",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJzMM0vDE0X",
        "colab_type": "text"
      },
      "source": [
        "# Ethical"
      ]
    }
  ]
}